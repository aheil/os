# Einheit 11: Harddisks & Dateisysteme

## Lernziele und Kompetenzen

Den Aufbau von Hard Disk Drives und RAID-Systemen **kennen lernen** und die Prinzipien bei der Ansteuerung durch das Betriebssystem **verstehen**.

## Datenpersistenz

* Hard Disk Drives (dt. Festplatten sind die seit Jahrzehnten am weit verbreitetsten Art Daten zu speichern.
* Dateisysteme hÃ¤ngen dabei stark von den darunterliegenden GerÃ¤ten ab
  * Wie speichern moderne Hard Disks Ã¼berhaupt Daten ab?
  * Wie sieht das Interface hierfÃ¼r aus?
  * Wie sind die Daten konkret angeordnet und wie wird darauf zugegriffen?
  * Wie lÃ¤sst sich mit â€Disk Schedulingâ€œ die Leistung verbessern?
  * Welche Konsequenz hat der Wandel von klassischen Festplatten hin zu Solid State Disks (Abk. SSD)?

### **Das Interface**

Der Aufbau ist im Grundprinzip immer Ã¤hnlich

* Das Laufwerk besteht aus einer Anzahl von sog. **Sektoren** (i.d.R. in Form von 512-Byte BlÃ¶cken)
* Jeder Block kann individuell gelesen und geschrieben werden
* Alle Sektoren sind nummeriert 0 bis ğ‘›âˆ’1 (bei ğ‘› Sektoren)
* Multi-Sektor-Operationen sing mÃ¶glich (und gÃ¤ngig)
* Viele Dateisysteme lesen 4KB oder mehr auf einmal
* Atomare Schreiboperationen sind nur auf 512-Byte BlÃ¶cke zugesichert

### Torn Write

Was bedeutet atomare Schreiboperationen sind nur auf 512-Byte BlÃ¶cken zugesichert?

<figure><img src=".gitbook/assets/os.11.torn_write.de.png" alt=""><figcaption></figcaption></figure>

Nur die ersten drei BlÃ¶cke wurden geschrieben, obwohl der Stromausfall erst sehr spÃ¤t bei der Schreiboperation von Block 4 aufgetreten ist.

### **Inoffizielle Annahmen Ã¼ber Disks**

*   Annahmen, die von vielen Clients getroffen werden (**unwritten contract**):

    * Auf zwei nahe beieinander liegende BlÃ¶cke kann schneller zugegriffen werden, als auf weit entfernt liegende
    * Der Zugriff auf fortlaufende BÃ¶cke (engl. sequential read/write) ist der schnellste Zugriff Ã¼berhaupt und gewÃ¶hnlich schneller als der wahlfreie Zugriff (engl. random access)



    > Angenommen, Sie schreiben einen Treiber fÃ¼r (konventionelle) Festplatten unter diesen Annahmen und morgen tauscht jemand die Festplatten gegen Solid State Disks aus, was passiert dann?

## Festplattengemoetrie

### **Grundlegende Geometrie**

* Eine oder mehrere Scheiben (engl. platter), jede mit je zwei Seiten
* Magnetische OberflÃ¤che aus Eisenoxid - oder Kobalt-Deckschicht (engl. surface)
* Achse bzw. Spindle (engl. spindle)
* Schreib-/Lesekopf (engl. disk-head)
* Arm mittels dem der Schreib-/Lesekopf positioniert wird (engl. disk arm)
* Daten sind in konzentrischen Kreisen (engl.tracks) angeordnet
* Umdrehung wird in RPM (rotations per minute) gemessen.
* Typische Werte heutzutage von 7.200 bis 15.000 RPM
* Interessant wird die Umdrehungszeit, bei 10.000 RPM sind dies ca. 6ms

### **Vereinfachte Festplatte**

* Einige (vereinfachende) Annahmen
  * Ein Track
  * Track besteht aus 12 Sektoren bzw. BlÃ¶cken (Sektoren)
  * Jeder Block besteht aus 512 Byte
  * Die Scheibe dreht sich gegen der Uhrzeiger Sinn

<figure><img src=".gitbook/assets/os.11.simple_disk.de.png" alt=""><figcaption></figcaption></figure>

### Rotationa Delay

* Rotational Delay oder auch Rotational Latency â€“ Zeit bis sich der gesuchte Sektor unter dem Schreib-Lese-Kopf befindet
* Eine vollstÃ¤ndige Umdrehung dauert ğ‘…
* Suchen wir Sektor 0 und starten bei Sektor 6, ist das Delay ğ‘…/2
* Der Worst-Case wÃ¤re im Beispiel zuvor ein Start bei 5, hier wird fast eine ganze Rotation benÃ¶tigt und das Delay betrÃ¤gt somit fast ğ‘…

### Seek Time

* In Wirklichkeit besitzen HDDs **sehr viele** Tracks und der Schreib-/Lesekopf muss permanent ausgerichtet werden
  * Hier: Kopf Ã¼ber dem innersten Track muss zum Ã¤uÃŸersten bewegt werden (engl. seek):
  * Rotation und Seek sind mit die teuersten Operationen einer Festplatte
  * Seeking besteht aus vier Phasen:
    * Beschleunigung (engl. acceleration)
    * Schub bei voller Geschwindigkeit (engl. coasting)
    * Abbremsung (engl. deceleration)
    * Einschwingzeit (engl. settling time) mit 0,5 bis 2ms

<figure><img src=".gitbook/assets/os.11.seek_time.de.png" alt=""><figcaption></figcaption></figure>

### **Transfer und andere unwichtige Dinge**

Erst wenn der Kopf korrekt positioniert ist (stellen Sie sich vor, er wÃ¤re nur ungefÃ¤hr auf dem richtigen TrackğŸ¤¦â€â™‚ï¸) findet der Transfer (engl.transfer) statt.

Um dass sequentielle Lesen zu ermÃ¶glichen, nutzen manche Disks ein sog. Spurversatz (engl. **trackskew**) an, damit keine Latenz nach dem Neupositionieren entsteht, wenn die Daten auf einem anderen Sektor weitergefÃ¼hrt werden.

AuÃŸen befinden sich mehr Sektoren (Physik rulez!), daher werden Platten oft in Zonen (engl. multi-zoned disks). Ã„uÃŸere Zonen besitzen dann mehr Sektoren als innere.

Schreib-/Lesecache zur Performance-Steigerung. Beim Schreiben kann sofort nach dem Cachen bestÃ¤tigt werden (engl. **writeback**) oder erst nach dem Schreiben auf Platte (engl. writet**h**rough).\


**I/O Zeiten**

Wie setzt sich nun die Zeit fÃ¼r einen I/O-Zugriff zusammen?

$$
T_{I/O}=T_{seek} +T_{rotation}+T_{transfer}
$$

FÃ¼r den Plattenvergleich gerne genutzt: I/O Rate:

$$
R_{I/O} = {\frac{Size_{transfer}}{T_{I/O}}}
$$

<figure><img src="https://github.com/aheil/os/raw/main/img/os.11.disc_spec.de.png" alt=""><figcaption></figcaption></figure>

## Disk Scheduling

* Aufgrund der hohen Kosten fÃ¼r Disk Zugriffe entscheidet der Disk Scheduler Ã¼ber die Zugriffe:
  * Anders als bei Prozessen kann man bei Plattenzugriffen die Dauer gut berechnen
  * Auf Basis von Seek-Zeiten und der Rotation Delay kann der kÃ¼rzeste Job gefunden werden

### **Shortest Seek Time First (SSTF)**

* Anordnung der Jobs nach Track â€“ die Anfrage mit dem am nÃ¤chst gelegenen Track wird zuerst gewÃ¤hlt
* Problem: Die Disk Geometrie ist dem Betriebssystem nicht bekannt
* Anstelle dessen kann der nÃ¤chst gelegen Block verwendet werden (nearest-block-first, Abk. NBF)
* Problem 2: Starvationâ€“ Bei einem fortlaufenden Strom von Anfragen auf z.B. die inneren Tracks wÃ¼rden Anfragen auf die Ã¤uÃŸeren ignoriert
* Wie kann dieses Problem gelÃ¶st werden?

### **SCAN**

* Anfragen werden von den Ã¤uÃŸeren zu den inneren Tracks und wieder zurÃ¼ck etc. abgearbeitet (engl. sweep)

### **C-SCAN (Circular SCAN)**

* Anstelle in beiden Richtungen werden Anfragen immer von den Ã¤uÃŸeren Tracks abgearbeitet
* Fairer gegenÃ¼ber den Ã¤uÃŸeren und inneren Tracks, da reines SCAN zweimal die mittleren Tracks trifft
* Allerdings werden SCAN/C-SCAN nicht annÃ¤hernd einem SJF-Ansatz gerecht

### **Shortest Positioning Time First (SPTF)**

* Ausgangspunkt s. vorherige Abbildung
* Sollte nun Track 8 oder 16 zuerst gewÃ¤hlt werden?
* AbhÃ¤ngig von Seek-Zeit und Rotation-Delay
* LÃ¶st eigentlich unsere vorherigen Probleme
* Problem: Das Betriebssystem kennt meist nicht die Track-Grenzen nicht und weiÃŸ nicht wo sich der Schreib-Lese-Kopf gerade befindet
* Daher wird SPFT meist innerhalb des Drives selbst implementiert

manchmal auch: Shortest Access Time First (SATF)

### **Weiter Herausforderungen**

* FrÃ¼her wurde das gesamte Scheduling im Betriebssystem realisiert â€“ frÃ¼her waren die Disks â€einfacherâ€œ gebaut.
* Heute besitzen Festplatten einen komplexen Scheduler auf dem Disk Controller, der exakte Daten Ã¼ber die internen Positionen hat.
* Das Betriebssystem schickt die Requests an die Disk, die es am geeignetsten hÃ¤lt und die Disk kÃ¼mmert sich um den Rest.
* I/O Merging: Requests, die nahe aneinander liegende Sektoren betreffen, sollten mÃ¶glichst zusammengefasst werden, da dies den Overhead fÃ¼r das Betriebssystem reduziert.
* Wie lange soll der Scheduler warten, bis eine I/O-Anfrage abgearbeitet wird? Es kÃ¶nnte ja noch eine â€bessereâ€œ Anfrage kommen, so dass die Disk effizienter genutzt werden kann.

## RAID-Systeme

Festplatten gehÃ¶ren zu den **langsamsten** Komponenten in einem Rechner. Wenn eine Festplatte ausfÃ¤llt, sind die persistierten Daten verloren. AuÃŸer Sie haben ein Backup, aber das ist hier nicht der Punkt, wicht hier ist jedoch: RAID ist kein Backup!

ZunÃ¤chst die Frage: Wie kann ein groÃŸes, schnelles und zuverlÃ¤ssiges Speichersystem geschaffen werden?

* Von auÃŸen betrachtet sieht ein RAID wie _eine_ Festplatte aus.
* Intern ist ein RAID jedoch ein hÃ¶chst komplexes System mit zahlreichen Vorteilen:
  * Performance, Speicherplatz (KapazitÃ¤t) und ZuverlÃ¤ssigkeit
  * RAID-Systeme verkraften auÃŸerdem den Ausfall einzelner Festplatten

### **Interface**

FÃ¼r das Dateisystem sieht ein RAI- System aus wie eine einzelne Festplatte (warum es das nicht ist klÃ¤ren wir spÃ¤ter).

* Bei einem Request durch das Betriebssystem, muss das RAID ermitteln auf welche Disk (bzw. abhÃ¤ngig vom RAID Level, auf welche Disks) zugegriffen werden muss.
* Da die Daten auf mehrere Disks verteilt sind, mÃ¼ssen mehrere physikalische I/O-Zugriffe pro logischen I/O-Zugriff stattfinden

### RAID Charakteristika

Auf Basis welcher Kriterien kÃ¶nnen RAID-Systeme evaluiert werden?

#### **KapazitÃ¤t**

* Wie viel effektiver Speicherplatz ist verfÃ¼gbar, wenn ğ‘ Disks mit ğµ BlÃ¶cken verwendet werden? Ohne Redundanz sind dies ğ‘â‹…ğµ
* Wenn zwei Kopien vorgehalten werden (engl. mirroring) wÃ¤ren dies (ğ‘â‹…ğµ)âˆ•2
* Verschiedene RAID-Level liegen irgendwo dazwischen

#### **ZuverlÃ¤ssigkeit**

* Zur Vereinfachung gehen wir derzeit von einem einzigen Fehlermodell aus: Eine Disk fÃ¤llt komplett aus, einem sog. Fail-Stop.
* Des weiteren gehen wir davon aus, dass der RAID-Controller dies auch direkt feststellen kann.
  * Wie viele Disks kÃ¶nnen ausfallen, so dass das jeweilige RAID-Design immer noch funktionsfÃ¤hig ist?

Es gibt natÃ¼rlich noch mehr FehlerfÃ¤lle, die wir spÃ¤ter betrachten!

#### **Performance**

* Die Performance ist nicht ganz einfach zu bestimmen:
  * HÃ¤ngt vom jeweiligen Workload ab
  * Wie hoch ist die Schreibe- oder Lesegeschwindigkeit?
  * Wie wir vorher gelernt haben, hÃ¤ngt dies auch von den eingesetzten Disks ab

### RAID Level

#### RAID Level 0

* Keine Redundanz
* Mehrere Disks werden genutzt, um die KapazitÃ¤t zu erhÃ¶hen (engl.striping)
* Einfachste Form: BlÃ¶cke werden Ã¼ber die Disks verteilt
* Werden BlÃ¶cke nun sequentiell gelesen, kann dies parallelisiert werden!

**Stripes**

BlÃ¶cke in der gleichen Reihe werden _Stripes_ genannt.

<figure><img src=".gitbook/assets/os.11.stripes.de.png" alt=""><figcaption></figcaption></figure>

**Chunk Size**

* Besser: Mehrere BlÃ¶cke auf einer Disk
* Hier: Zwei 4-KB BlÃ¶cke bevor zur nÃ¤chsten Disk gesprungen wird

<figure><img src=".gitbook/assets/os.11.many_stripes.de.png" alt=""><figcaption></figcaption></figure>

* Performance Auswirkung:
  * Kleine Chunk Sizes: Dateien werden Ã¼ber viele Disks verteilt
  * GroÃŸe Chunk Sizes: Intra-File ParallelitÃ¤t wird reduziert
  * Richtige GrÃ¶ÃŸe: schwer zu bestimmen bzw. â€it dependsâ€œ

**RAID-0 Analyse**

**KapazitÃ¤t**

* Bei ğ‘ Disk mit je ğµ BlÃ¶cken liefert RAID-0 ein perfektes Ergebnis: ğ‘â‹…ğµ

**ZuverlÃ¤ssigkeit**

* Perfekt, was die Ausfallwahrscheinlichkeit angeht: Bei einem Fehler sind die Daten futsch!

**Performance**

* Bei einem Zugriff auf einen einzelnen Block: Vergleichbar mit einzelner Disk
* Bei sequentiellen Zugriffen: Volle ParallelitÃ¤t
* Bei wahlfreien Zugriffen ğ‘â‹…ğ‘… MB/s mit&#x20;

$$
ğ‘…=(ğ´ğ‘šğ‘œğ‘¢ğ‘›ğ‘¡ğ‘œğ‘“ğ·ğ‘ğ‘¡ğ‘)/(ğ‘‡ğ‘–ğ‘šğ‘’ğ‘¡ğ‘œğ´ğ‘ğ‘ğ‘’ğ‘ ğ‘ )
$$

FÃ¼r eine detaillierte Berechnung sei hier auf OSTEP Kapitel 38.4 verwiesen

### RAID Level 1

**Mirroring**

* Jeder Block wird im System auf eine andere Disk kopiert (bzw. gespiegelt)
* Hier: RAID-10 bzw. RAID 1+0, nutzt gespiegelte Paare von Disk
* Alternativ: RAID-01 bzw. RAID 0+1, besteht aus zwei RAID-0 Arrays, die gespiegelt sind

<figure><img src=".gitbook/assets/os.11.raid-1.de.png" alt=""><figcaption></figcaption></figure>

**KapazitÃ¤t**

* Es wird nur die HÃ¤lfte der KapazitÃ¤t genutzt: (ğ‘â‹…ğµ)âˆ•2 und somit teuer

**ZuverlÃ¤ssigkeit**

* Ausfall einer Diks wird verkraftet, im vorherigen Fall kÃ¶nnen sogar Konstellationen von Disks ausfallen (z.B. Disk 0 und 2), darauf sollte man aber nicht wetten

**Performance**

* Einzelne Leseoperation vergleichbar mit einer einzelnen Disk
* FÃ¼r einen Schreibzugriff mÃ¼ssen jedoch zwei (parallele) physikalische Schreiboperationen durchgefÃ¼hrt werden, im Worst-Case muss auf den langsamsten Schreibprozess gewartet werden (z.B. aufgrund von Rotation Delay)
* Sequentielle Schreib- und Leseoperationen dauern (ğ‘/2â‹…ğ‘†) MB/s mit ğ‘†=(ğ´ğ‘šğ‘œğ‘¢ğ‘›ğ‘¡ğ‘œğ‘“ğ·ğ‘ğ‘¡ğ‘)/(ğ‘‡ğ‘–ğ‘šğ‘’ğ‘¡ğ‘œğ´ğ‘ğ‘ğ‘’ğ‘ ğ‘ ) bzw. die HÃ¤lfte des HÃ¶chstdurchsatzes
* Wahlfreie Leseoperationen sind mit ğ‘â‹…ğ‘… MB/s die beste Operation fÃ¼r RAID-1, wogegen wahlfreie Schreiboperationen mit ğ‘/2â‹…ğ‘… MB/s weniger geeignet sind, da zwei physikalische Schreiboperationen simultan durchgefÃ¼hrt werden mÃ¼ssen.

FÃ¼r eine detaillierte Berechnung sei auch hier auf OSTEP Kapitel 38.4 verwiesen

#### RAID Level 4

* Nutzung eines sog ParitÃ¤tsbits
* BenÃ¶tigt weniger Speicherplatz als gespiegelte RAIDs, jedoch auf Kosten der Performance
* Mittels der **XOR**-Funktion wird das ParitÃ¤tsbit berechnet

<figure><img src=".gitbook/assets/os.11.parity_bit.de.png" alt=""><figcaption></figcaption></figure>

**Parity-Bit**

* Invariante
* Pro Zeile gerade Anzahl von 1en, einschl. des ParitÃ¤tsbits
* RAID muss dies sicherstellen
* Beim Ausfall einer Zeile C (s.o.) kann diese wiederhergestellt werden
  * Wie? XOR auf die verbleibenden Spalten ausfÃ¼hren
* Aber bei BlÃ¶cken?
* Bitweises XOR auf den ganzen Block (z.B. 4 KB)

<figure><img src=".gitbook/assets/os.11.xor.de.png" alt=""><figcaption></figcaption></figure>

**KapazitÃ¤t**

* 1 Disk fÃ¼r ParitÃ¤ten ergibt eine GesamtkapazitÃ¤t (ğ‘âˆ’1)â‹…ğµ

**ZuverlÃ¤ssigkeit**

* RAID-1 erlaubt den Ausfall einer Disk

**Performance**

* Sequentielle Leseoperationen kÃ¶nnen alle Disks (ohne die ParitÃ¤tsdisk) nutzen und liefern so einen Maximaldurchsatz von (ğ‘âˆ’1)â‹…ğ‘† MB/s
* Bei einem sog. Full Stripe Write wird ein gesamter Stripe auf einmal beschrieben und der ParitÃ¤tsblock kann direkt mit berechnet werden, alle Schreiboperationen kÃ¶nnen parallel stattfinden (effizienteste Schreiboperation im RAID-4)
* Die effektive Bandbreite bei sequentiellen Schreiboperationen ist dabei (ğ‘âˆ’1)â‹…ğ‘† MB/s
* Wahlfreie Leseoperationen liegen bei (ğ‘âˆ’1)â‹…ğ‘… MB/s
* Beim Schreiben eines einzelnen Blocks muss das ParitÃ¤tsbit des Stripes neu berechnet werden

**Variante 1: Additive Parity**

* Alle bestehenden BlÃ¶cke (parallel) lesen und mit dem neune Block `XOR`
* Neu berechneter ParitÃ¤tsblock und neuer Block kÃ¶nnen parallel geschrieben werden

**Variante 2: Subtractive Parity**

* Alter Wert wird gelesen, ist dieser mit dem neuen Wert identisch muss das ParitÃ¤tsbit nicht geÃ¤ndert werden, falls doch, muss das ParitÃ¤tsbit umgedreht werden
* Bei ganzen BlÃ¶cken (z.B. 4 KB) wie in RAID-4 sind dies 4096 mal 8 Bit.
* Der Einsatz des jeweiligen Verfahrens hÃ¤ngt also wieder davon ab (â€it dependsâ€œ)

> Auf jeden Fall wird die ParitÃ¤tsdisk zum Flaschenhals

RAID Level 5

* Grundlegend gleich zu RAID-4, jedoch mit den ParitÃ¤tsblÃ¶cken Ã¼ber die versch. Disks verteilt (engl. rotating parity)
* Flaschenhals wird somit beseitigt

<figure><img src=".gitbook/assets/os.11.rotating_parity.de.png" alt=""><figcaption></figcaption></figure>

**RAID-5 Analyse**

* Die meisten Werte sind identisch zu RAID-4
* Wahlfreie Leseoperationen sind etwas besser, da alle Disks genutzt werden kÃ¶nnen
* Wahlfreie Schreiboperationen verbessern sich signifikant, da Requests nun parallel ausgefÃ¼hrt werden kÃ¶nnen.

## Ãœbungsaufgaen

### Einfache XOR Berechnung

Gegeben sind zwei BinÃ¤rzahlen: 1010 und 1100. Berechnen Sie das Ergebnis der XOR-Operation zwischen diesen beiden Zahlen.

1 XOR 1 = 0\
0 XOR 1 = 1\
1 XOR 0 = 1\
0 XOR 0 = 0

Die gesuchte BinÃ¤rzahl ist demnach 0110.\
\
In diesem Fall handelt es sich um **gerade ParitÃ¤t** (auch als â€œeven parityâ€ bezeichnet). Die XOR-Operation zwischen den beiden BinÃ¤rzahlen 1010 und 1100 ergibt die BinÃ¤rzahl 0110, die eine gerade Anzahl von Einsen enthÃ¤lt. Daher ist dies ein Beispiel fÃ¼r gerade ParitÃ¤t.&#x20;

### XOR Berechnung mit mehreren DatenblÃ¶cken

* **Disk 1 =**1111
* **Disk 2 =**1110
* **Disk 3 =**1100
* **Disk 4 =**1000

**Die Berechnung des ersten Bits jedes Datenblocks ergibt folgende Berechnung**

1 XOR 1 XOR 1 XOR 1&#x20;

\= ((1 XOR 1) XOR 1) XOR 1

\= (0 XOR 1) XOR 1

\= 1 XOR 1n

\= 0

**Die Berechnung des zweiten Bits jedes Datenblocks ergibt folgende Berechnung**

1 XOR 1 XOR 1 XOR 0&#x20;

\= ((1 XOR 1) XOR 1) XOR 0

\= (0 XOR 1) XOR 0

\= 1 XOR 0

\= 1

**Die Berechnung des dritten Bits jedes Datenblocks ergibt folgende Berechnung**

1 XOR 1 XOR 0 XOR 0&#x20;

\= ((1 XOR 1) XOR 0) XOR 0

\= (0 XOR 0) XOR 0

\= 0 XOR 0

\= 0

**Die Berechnung des letzten Bits jedes Datenblocks ergibt folgende Berechnung**

1 XOR 0 XOR 0 XOR 0&#x20;

\= ((1 XOR 0) XOR 0) XOR 0

\= (1 XOR 0) XOR 0

\= 1 XOR 0

\= 1

Die Berechnung ergibt demnach folgenden ParitÃ¤tsblock:

**Disk 5 =** 0101

